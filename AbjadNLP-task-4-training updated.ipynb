{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-24T13:37:47.261695Z",
     "iopub.status.busy": "2025-12-24T13:37:47.261440Z",
     "iopub.status.idle": "2025-12-24T13:37:47.267017Z",
     "shell.execute_reply": "2025-12-24T13:37:47.266398Z",
     "shell.execute_reply.started": "2025-12-24T13:37:47.261678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# EACL 2026 Abjad NLP: Medical Text Classification with AraBERT v2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch. utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn. metrics import f1_score, classification_report, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:38:01.658201Z",
     "iopub.status.busy": "2025-12-24T13:38:01.657695Z",
     "iopub.status.idle": "2025-12-24T13:38:02.014863Z",
     "shell.execute_reply": "2025-12-24T13:38:02.014233Z",
     "shell.execute_reply.started": "2025-12-24T13:38:01.658176Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EACL 2026 ABJAD NLP - ARABERT V2 TRAINING PIPELINE\n",
      "\n",
      "ğŸ–¥ï¸  Device Information:\n",
      "   PyTorch version: 2.6.0+cu124\n",
      "   CUDA available: True\n",
      "   CUDA device: Tesla T4\n",
      "   CUDA memory: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATION\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "\n",
    "# MODEL_NAME = \"aubmindlab/bert-base-arabertv2\"\n",
    "# MODEL_NAME = \"aubmindlab/bert-base-arabertv02\"  \n",
    "# MODEL_NAME = \"CAMeL-Lab/bert-base-arabic-camelbert-mix\"  \n",
    "# MODEL_NAME = \"xlm-roberta-base\"  \n",
    "# MODEL_NAME = \"UBC-NLP/MARBERTv2\" \n",
    "# MODEL_NAME = \"aubmindlab/bert-large-arabertv02\"\n",
    "\n",
    "\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"aubmindlab/bert-base-arabertv02\" \n",
    "NUM_LABELS = 82\n",
    "MAX_LENGTH = 384\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 10\n",
    "WARMUP_STEPS = 500\n",
    "\n",
    "# Clear memory\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "BASE_DIR = '/kaggle/working'\n",
    "OUTPUT_DIR = f'{BASE_DIR}/results'           # Training checkpoints\n",
    "MODEL_SAVE_DIR = f'{BASE_DIR}/arabert_medical_model'  # Final model\n",
    "LOGS_DIR = f'{BASE_DIR}/logs'                # Training logs\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EACL 2026 ABJAD NLP - ARABERT V2 TRAINING PIPELINE\")\n",
    "\n",
    "print(f\"\\nğŸ–¥ï¸  Device Information:\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA memory: {torch.cuda. get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:38:12.104069Z",
     "iopub.status.busy": "2025-12-24T13:38:12.103512Z",
     "iopub.status.idle": "2025-12-24T13:38:12.563287Z",
     "shell.execute_reply": "2025-12-24T13:38:12.562517Z",
     "shell.execute_reply.started": "2025-12-24T13:38:12.104044Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[1/12] LOADING DATA\n",
      "================================================================================\n",
      "\n",
      "âœ… Data loaded successfully!\n",
      "   Shape: (27951, 3)\n",
      "   Columns: ['text', 'category', 'label']\n",
      "\n",
      "First few rows:\n",
      "                                                text                category  \\\n",
      "0  Ø§Ù„Ø³Ø¤Ø§Ù„\\n-------\\nØ§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù†Ø§ Ù…ØµØ§Ø¨ Ø¨ÙÙ‚Ø± Ø§Ù„...  Hematological diseases   \n",
      "1  Ø§Ù„Ø³Ø¤Ø§Ù„\\n-------\\nØ§Ù†Ø§ Ø´Ø§Ø¨ Ø¹Ù†Ø¯Ù‰ 25 Ø³Ù†Ù‡ ÙˆØ¹Ù†Ø¯Ù‰ ØªØ¨Ùˆ...     Urogenital diseases   \n",
      "2  Ø§Ù„Ø³Ø¤Ø§Ù„\\n-------\\nØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ± Ø¹Ù†Ø¯ÙŠ Ø§Ù„Ù‚Ø¶ÙŠØ¨ ØºÙŠØ± Ù†Ø´...         Medicinal herbs   \n",
      "3  Ø§Ù„Ø³Ø¤Ø§Ù„\\n-------\\nÙ‡Ù„ ÙŠØ¸Ù‡Ø± Ø§Ù„Ø­Ø´ÙŠØ´ ÙÙŠ ØªØ­Ù„ÙŠÙ„ CBC Ùˆ...               Addiction   \n",
      "4  Ø§Ù„Ø³Ø¤Ø§Ù„\\n-------\\nÙˆØ²Ù†ÙŠ 58 ÙƒØºÙ… ÙˆØ§Ø±ÙŠØ¯ Ø§Ù† Ø§ÙÙ‚Ø¯ 5 Ùƒ...                 Biology   \n",
      "\n",
      "   label  \n",
      "0     33  \n",
      "1     76  \n",
      "2     45  \n",
      "3      0  \n",
      "4      7  \n",
      "\n",
      "âœ… Data validation passed!\n"
     ]
    }
   ],
   "source": [
    "# 1. LOAD DATA\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[1/12] LOADING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Update this path to your training data\n",
    "train_df = pd.read_csv('/kaggle/input/arabic/shared_task_train.csv')\n",
    "\n",
    "print(f\"\\nâœ… Data loaded successfully!\")\n",
    "print(f\"   Shape: {train_df.shape}\")\n",
    "print(f\"   Columns: {train_df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(train_df.head())\n",
    "\n",
    "# Quick validation\n",
    "assert 'text' in train_df. columns, \"Missing 'text' column\"\n",
    "assert 'label' in train_df.columns, \"Missing 'label' column\"\n",
    "assert train_df['label'].min() == 0, \"Labels should start from 0\"\n",
    "assert train_df['label'].max() == 81, \"Labels should go up to 81\"\n",
    "print(\"\\nâœ… Data validation passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:38:19.029422Z",
     "iopub.status.busy": "2025-12-24T13:38:19.028725Z",
     "iopub.status.idle": "2025-12-24T13:38:19.050923Z",
     "shell.execute_reply": "2025-12-24T13:38:19.050049Z",
     "shell.execute_reply.started": "2025-12-24T13:38:19.029394Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[2/12] ANALYZING CLASS DISTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "Class Statistics:\n",
      "   Unique classes: 82\n",
      "   Mean samples/class: 340.87\n",
      "   Median samples/class: 350.50\n",
      "   Min samples:  7\n",
      "   Max samples: 600\n",
      "   Imbalance ratio: 85.71x\n"
     ]
    }
   ],
   "source": [
    "# 2. ANALYZE CLASS DISTRIBUTION\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2/12] ANALYZING CLASS DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "label_counts = train_df['label'].value_counts().sort_index()\n",
    "\n",
    "print(f\"\\nClass Statistics:\")\n",
    "print(f\"   Unique classes: {train_df['label'].nunique()}\")\n",
    "print(f\"   Mean samples/class: {label_counts.mean():.2f}\")\n",
    "print(f\"   Median samples/class: {label_counts.median():.2f}\")\n",
    "print(f\"   Min samples:  {label_counts.min()}\")\n",
    "print(f\"   Max samples: {label_counts.max()}\")\n",
    "print(f\"   Imbalance ratio: {label_counts.max() / label_counts.min():.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:38:23.309208Z",
     "iopub.status.busy": "2025-12-24T13:38:23.308908Z",
     "iopub.status.idle": "2025-12-24T13:38:23.316190Z",
     "shell.execute_reply": "2025-12-24T13:38:23.315385Z",
     "shell.execute_reply.started": "2025-12-24T13:38:23.309184Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CLASS DISTRIBUTION (Ascending Order by Class ID)\n",
      "================================================================================\n",
      "Class  0:  600 samples\n",
      "Class  1:  333 samples\n",
      "Class  2:  232 samples\n",
      "Class  3:   37 samples\n",
      "Class  4:   34 samples\n",
      "Class  5:  600 samples\n",
      "Class  6:    7 samples\n",
      "Class  7:   29 samples\n",
      "Class  8:  345 samples\n",
      "Class  9:  600 samples\n",
      "Class  10:   11 samples\n",
      "Class  11:  600 samples\n",
      "Class  12:   13 samples\n",
      "Class  13:  600 samples\n",
      "Class  14:  600 samples\n",
      "Class  15:  600 samples\n",
      "Class  16:  600 samples\n",
      "Class  17:  600 samples\n",
      "Class  18:  154 samples\n",
      "Class  19:  600 samples\n",
      "Class  20:   40 samples\n",
      "Class  21:  600 samples\n",
      "Class  22:  600 samples\n",
      "Class  23:   41 samples\n",
      "Class  24:  600 samples\n",
      "Class  25:  600 samples\n",
      "Class  26:  600 samples\n",
      "Class  27:  100 samples\n",
      "Class  28:   26 samples\n",
      "Class  29:   10 samples\n",
      "Class  30:  156 samples\n",
      "Class  31:  600 samples\n",
      "Class  32:  600 samples\n",
      "Class  33:  600 samples\n",
      "Class  34:   11 samples\n",
      "Class  35:  144 samples\n",
      "Class  36:  600 samples\n",
      "Class  37:   55 samples\n",
      "Class  38:    7 samples\n",
      "Class  39:  242 samples\n",
      "Class  40:  232 samples\n",
      "Class  41:  600 samples\n",
      "Class  42:  411 samples\n",
      "Class  43:  134 samples\n",
      "Class  44:   46 samples\n",
      "Class  45:  241 samples\n",
      "Class  46:  600 samples\n",
      "Class  47:  600 samples\n",
      "Class  48:   29 samples\n",
      "Class  49:  600 samples\n",
      "Class  50:  600 samples\n",
      "Class  51:  356 samples\n",
      "Class  52:  600 samples\n",
      "Class  53:   49 samples\n",
      "Class  54:  600 samples\n",
      "Class  55:  600 samples\n",
      "Class  56:   34 samples\n",
      "Class  57:  600 samples\n",
      "Class  58:   10 samples\n",
      "Class  59:  600 samples\n",
      "Class  60:   20 samples\n",
      "Class  61:  251 samples\n",
      "Class  62:  600 samples\n",
      "Class  63:  600 samples\n",
      "Class  64:   20 samples\n",
      "Class  65:  600 samples\n",
      "Class  66:  229 samples\n",
      "Class  67:  600 samples\n",
      "Class  68:   50 samples\n",
      "Class  69:    8 samples\n",
      "Class  70:  600 samples\n",
      "Class  71:   20 samples\n",
      "Class  72:  600 samples\n",
      "Class  73:  600 samples\n",
      "Class  74:  600 samples\n",
      "Class  75:   33 samples\n",
      "Class  76:  600 samples\n",
      "Class  77:  235 samples\n",
      "Class  78:   19 samples\n",
      "Class  79:    7 samples\n",
      "Class  80:   90 samples\n",
      "Class  81:  600 samples\n"
     ]
    }
   ],
   "source": [
    "# Quick class distribution check\n",
    "class_counts = train_df['label'].value_counts().sort_index()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CLASS DISTRIBUTION (Ascending Order by Class ID)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show all classes in ascending order\n",
    "for class_id, count in class_counts.items():\n",
    "    print(f\"Class {class_id: 2d}: {count:4d} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:38:30.636460Z",
     "iopub.status.busy": "2025-12-24T13:38:30.635714Z",
     "iopub.status.idle": "2025-12-24T13:38:31.842122Z",
     "shell.execute_reply": "2025-12-24T13:38:31.841475Z",
     "shell.execute_reply.started": "2025-12-24T13:38:30.636435Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[3/12] PREPROCESSING ARABIC TEXT\n",
      "================================================================================\n",
      "\n",
      "Applying preprocessing...\n",
      "âœ… Preprocessing complete!\n",
      "\n",
      "Example:\n",
      "Original: Ø§Ù„Ø³Ø¤Ø§Ù„\n",
      "-------\n",
      "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù†Ø§ Ù…ØµØ§Ø¨ Ø¨ÙÙ‚Ø± Ø§Ù„Ø¯Ù… Ø§Ù„Ù…Ù†Ø¬Ù„ÙŠ (Ø§Ù„Ø³ÙƒÙ„Ø³Ù„) Ø¹Ù„Ù…Ø¢ Ø¨Ø£Ù† Ù†Ø³Ø¨Ø© Ø§Ù„Ø³ÙƒÙ„Ø³Ù„ 72 ÙØ¹Ù†Ø¯Ù…Ø§ ØªØµØ¨Ø­ Ù†Ø³Ø¨Ø© Ø§Ù„Ø¯Ù… 7 ÙØ£Ù† Ø§Ù„Ø§Ù„Ø§Ù… ØªØ£ØªÙŠ Ø¨ÙƒØ«Ø±Ù‡ ÙÙ…Ø§ Ø§Ù„Ø­Ù„ Ù„Ø²ÙŠØ§Ø¯\n",
      "Cleaned:   Ø§Ù„Ø³Ø¤Ø§Ù„ ------- Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø§Ù†Ø§ Ù…ØµØ§Ø¨ Ø¨ÙÙ‚Ø± Ø§Ù„Ø¯Ù… Ø§Ù„Ù…Ù†Ø¬Ù„ÙŠ (Ø§Ù„Ø³ÙƒÙ„Ø³Ù„) Ø¹Ù„Ù…Ø§ Ø¨Ø§Ù† Ù†Ø³Ø¨Ù‡ Ø§Ù„Ø³ÙƒÙ„Ø³Ù„ 72 ÙØ¹Ù†Ø¯Ù…Ø§ ØªØµØ¨Ø­ Ù†Ø³Ø¨Ù‡ Ø§Ù„Ø¯Ù… 7 ÙØ§Ù† Ø§Ù„Ø§Ù„Ø§Ù… ØªØ§ØªÙŠ Ø¨ÙƒØ«Ø±Ù‡ ÙÙ…Ø§ Ø§Ù„Ø­Ù„ Ù„Ø²ÙŠØ§Ø¯\n"
     ]
    }
   ],
   "source": [
    "# 3. TEXT PREPROCESSING\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[3/12] PREPROCESSING ARABIC TEXT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def preprocess_arabic_text(text):\n",
    "    \"\"\"Preprocess Arabic text for medical classification\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove diacritics\n",
    "    text = re.sub(r'[Ù‹ÙŒÙÙÙÙÙ‘Ù’]', '', text)\n",
    "    \n",
    "    # Normalize Arabic letters\n",
    "    text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)\n",
    "    text = re.sub(r'Ù‰', 'ÙŠ', text)\n",
    "    text = re.sub(r'Ø©', 'Ù‡', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"\\nApplying preprocessing...\")\n",
    "train_df['text_clean'] = train_df['text'].apply(preprocess_arabic_text)\n",
    "\n",
    "print(\"âœ… Preprocessing complete!\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"Original: {train_df['text']. iloc[0][: 150]}\")\n",
    "print(f\"Cleaned:   {train_df['text_clean'].iloc[0][:150]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:38:38.678816Z",
     "iopub.status.busy": "2025-12-24T13:38:38.678531Z",
     "iopub.status.idle": "2025-12-24T13:38:38.686577Z",
     "shell.execute_reply": "2025-12-24T13:38:38.685708Z",
     "shell.execute_reply.started": "2025-12-24T13:38:38.678796Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PER-CLASS SAMPLE DISTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Summary:\n",
      "   Total classes: 82\n",
      "   Total samples: 27,951\n",
      "   Mean samples/class: 340.87\n",
      "   Median samples/class: 350.50\n",
      "   Min samples:  7 (Class 6)\n",
      "   Max samples: 600 (Class 0)\n",
      "   Imbalance ratio: 85.71x\n"
     ]
    }
   ],
   "source": [
    "# VIEW PER-CLASS SAMPLE DISTRIBUTION\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Analyze class distribution\n",
    "class_counts = train_df['label'].value_counts().sort_index()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PER-CLASS SAMPLE DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nğŸ“Š Summary:\")\n",
    "print(f\"   Total classes: {len(class_counts)}\")\n",
    "print(f\"   Total samples: {len(train_df):,}\")\n",
    "print(f\"   Mean samples/class: {class_counts.mean():.2f}\")\n",
    "print(f\"   Median samples/class: {class_counts.median():.2f}\")\n",
    "print(f\"   Min samples:  {class_counts.min()} (Class {class_counts.idxmin()})\")\n",
    "print(f\"   Max samples: {class_counts.max()} (Class {class_counts.idxmax()})\")\n",
    "print(f\"   Imbalance ratio: {class_counts.max() / class_counts.min():.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:38:45.800111Z",
     "iopub.status.busy": "2025-12-24T13:38:45.799833Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA AUGMENTATION - BACK-TRANSLATION METHOD\n",
      "================================================================================\n",
      "\n",
      "Loading translation models...\n",
      "  [1/2] Loading Arabic â†’ English...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1cd0485308a493ab54da5a2c72ae3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631a857fef3541e5b24622e52377bcf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/917k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1f7bca7ca945e0adef7955a04e3af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d11b79d3c5147a393d3a8ba4b5ff186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62f37aa3d2748729f279ad793cf0d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4965fe73b9cf4b278432be97745b390c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e176d20bee5947a08eed8ba3eb4a773b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [2/2] Loading English â†’ Arabic...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557f0bb516f240ad8dbddfca29db0337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc96b62994fc41ea9b36f7f4122a9248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/801k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478c4e2c1c454ada800a2f67cb04fa5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f100873b7d1a4ee1b5d0d2b75722cf5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/917k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929cade4c263400c84e5ea0b0abb6a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e589c6ad93644e9abd1f36698172f4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5352218dcf734aee9fe08ed253edc669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99d529ffffd4c11ac80c8631de7a361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0af1381dc54a40b439565d07e01f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Translation models loaded on cuda\n",
      "\n",
      "ğŸ“Š Found 24 minority classes (< 50 samples)\n",
      "   Minority classes: [3, 4, 6, 7, 10, 12, 20, 23, 28, 29, 34, 38, 44, 48, 53, 56, 58, 60, 64, 69, 71, 75, 78, 79]\n",
      "   Will augment each to 150 samples\n",
      "\n",
      "ğŸ”„ Using back-translation (Arabic â†’ English â†’ Arabic)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fae9522bced42ad9dd2126a864fe821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Augmenting classes:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DATA AUGMENTATION - BACK-TRANSLATION METHOD (BEST QUALITY)\n",
    "\n",
    "def augment_minority_classes(train_df, min_samples=50, target_samples=150):\n",
    "    \"\"\"\n",
    "    Augment minority classes using back-translation (Arabic â†’ English â†’ Arabic)\n",
    "    \n",
    "    Args:\n",
    "        train_df: DataFrame with 'text' and 'label' columns\n",
    "        min_samples:  Classes below this get augmented\n",
    "        target_samples:  Target samples per minority class\n",
    "    \n",
    "    Returns: \n",
    "        Augmented DataFrame (same name:  train_df)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATA AUGMENTATION - BACK-TRANSLATION METHOD\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    from transformers import MarianMTModel, MarianTokenizer\n",
    "    from tqdm.auto import tqdm\n",
    "    import random\n",
    "    import torch\n",
    "    \n",
    "    # Load translation models\n",
    "    print(\"\\nLoading translation models...\")\n",
    "    print(\"  [1/2] Loading Arabic â†’ English...\")\n",
    "    \n",
    "    try:\n",
    "        # Arabic to English\n",
    "        ar_en_model_name = 'Helsinki-NLP/opus-mt-ar-en'\n",
    "        ar_en_tokenizer = MarianTokenizer.from_pretrained(ar_en_model_name)\n",
    "        ar_en_model = MarianMTModel.from_pretrained(ar_en_model_name)\n",
    "        \n",
    "        # English to Arabic\n",
    "        print(\"  [2/2] Loading English â†’ Arabic...\")\n",
    "        en_ar_model_name = 'Helsinki-NLP/opus-mt-en-ar'\n",
    "        en_ar_tokenizer = MarianTokenizer.from_pretrained(en_ar_model_name)\n",
    "        en_ar_model = MarianMTModel.from_pretrained(en_ar_model_name)\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        ar_en_model = ar_en_model.to(device)\n",
    "        en_ar_model = en_ar_model. to(device)\n",
    "        \n",
    "        use_backtranslation = True\n",
    "        print(f\"âœ… Translation models loaded on {device}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        use_backtranslation = False\n",
    "        print(f\"âš ï¸  Translation models failed: {e}\")\n",
    "        print(\"   Falling back to fast augmentation...\")\n",
    "    \n",
    "    # Augmentation functions\n",
    "    def back_translate(text):\n",
    "        \"\"\"Arabic â†’ English â†’ Arabic back-translation\"\"\"\n",
    "        try: \n",
    "            # Translate to English\n",
    "            inputs = ar_en_tokenizer(text, return_tensors=\"pt\", padding=True, \n",
    "                                    truncation=True, max_length=512).to(device)\n",
    "            translated = ar_en_model.generate(**inputs, max_length=512, num_beams=4, \n",
    "                                             early_stopping=True)\n",
    "            english_text = ar_en_tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Translate back to Arabic\n",
    "            inputs = en_ar_tokenizer(english_text, return_tensors=\"pt\", padding=True, \n",
    "                                    truncation=True, max_length=512).to(device)\n",
    "            back_translated = en_ar_model.generate(**inputs, max_length=512, num_beams=4,\n",
    "                                                   early_stopping=True)\n",
    "            arabic_text = en_ar_tokenizer.decode(back_translated[0], skip_special_tokens=True)\n",
    "            \n",
    "            return arabic_text if arabic_text. strip() else text\n",
    "        except: \n",
    "            return text  # Return original if translation fails\n",
    "    \n",
    "    def fast_augment(text):\n",
    "        \"\"\"Fast fallback - swap words\"\"\"\n",
    "        words = text.split()\n",
    "        if len(words) < 2:\n",
    "            return text\n",
    "        \n",
    "        # Swap 1-2 random word pairs\n",
    "        n_swaps = random.randint(1, min(2, len(words)//2))\n",
    "        for _ in range(n_swaps):\n",
    "            idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "            words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "        \n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def synonym_replacement(text, n=2):\n",
    "        \"\"\"Simple synonym replacement for medical terms\"\"\"\n",
    "        words = text.split()\n",
    "        if len(words) < 3:\n",
    "            return text\n",
    "        \n",
    "        # Arabic medical synonyms\n",
    "        synonyms = {\n",
    "            'Ù…Ø±ÙŠØ¶': ['Ù…ØµØ§Ø¨', 'Ø¹Ù„ÙŠÙ„', 'Ø³Ù‚ÙŠÙ…'],\n",
    "            'Ø¹Ù„Ø§Ø¬': ['Ø¯ÙˆØ§Ø¡', 'Ù…Ø¹Ø§Ù„Ø¬Ø©', 'Ø·Ø¨'],\n",
    "            'Ø·Ø¨ÙŠØ¨': ['Ø¯ÙƒØªÙˆØ±', 'Ù…Ø¹Ø§Ù„Ø¬'],\n",
    "            'Ø£Ù„Ù…': ['ÙˆØ¬Ø¹', 'Ù…Ø¹Ø§Ù†Ø§Ø©'],\n",
    "            'Ø­Ø§Ù„Ø©': ['ÙˆØ¶Ø¹', 'Ø¸Ø±Ù'],\n",
    "            'ØµØ­Ø©': ['Ø¹Ø§ÙÙŠØ©', 'Ø³Ù„Ø§Ù…Ø©'],\n",
    "            'Ù…Ø±Ø¶': ['Ø¯Ø§Ø¡', 'Ø¹Ù„Ø©'],\n",
    "            'ÙØ­Øµ': ['Ø§Ø®ØªØ¨Ø§Ø±', 'ÙƒØ´Ù'],\n",
    "            'Ø£Ø¹Ø±Ø§Ø¶': ['Ø¹Ù„Ø§Ù…Ø§Øª', 'Ù…Ø¸Ø§Ù‡Ø±'],\n",
    "        }\n",
    "        \n",
    "        new_words = words.copy()\n",
    "        indices = random.sample(range(len(words)), min(n, len(words)))\n",
    "        \n",
    "        for idx in indices:\n",
    "            word = words[idx]\n",
    "            if word in synonyms:\n",
    "                new_words[idx] = random.choice(synonyms[word])\n",
    "        \n",
    "        return ' '.join(new_words)\n",
    "    \n",
    "    # Identify minority classes\n",
    "    class_counts = train_df['label'].value_counts()\n",
    "    minority_classes = class_counts[class_counts < min_samples]. index.tolist()\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Found {len(minority_classes)} minority classes (< {min_samples} samples)\")\n",
    "    print(f\"   Minority classes: {sorted(minority_classes)}\")\n",
    "    print(f\"   Will augment each to {target_samples} samples\")\n",
    "    \n",
    "    if use_backtranslation: \n",
    "        print(f\"\\nğŸ”„ Using back-translation (Arabic â†’ English â†’ Arabic)\")\n",
    "    else:\n",
    "        print(f\"\\nâš¡ Using fast augmentation (word swap + synonyms)\")\n",
    "    \n",
    "    # Augment\n",
    "    augmented_rows = []\n",
    "    \n",
    "    for class_id in tqdm(minority_classes, desc=\"Augmenting classes\"):\n",
    "        class_df = train_df[train_df['label'] == class_id]\n",
    "        current_count = len(class_df)\n",
    "        needed = target_samples - current_count\n",
    "        \n",
    "        if needed <= 0:\n",
    "            continue\n",
    "        \n",
    "        # Use text_clean if available, otherwise use text\n",
    "        text_col = 'text_clean' if 'text_clean' in train_df.columns else 'text'\n",
    "        texts = class_df[text_col].tolist()\n",
    "        \n",
    "        for i in range(needed):\n",
    "            original_text = random.choice(texts)\n",
    "            \n",
    "            # Apply augmentation with mixed methods\n",
    "            if use_backtranslation:\n",
    "                # 60% back-translation, 30% synonym, 10% swap\n",
    "                rand = random.random()\n",
    "                if rand < 0.6:\n",
    "                    augmented_text = back_translate(original_text)\n",
    "                elif rand < 0.9:\n",
    "                    augmented_text = synonym_replacement(original_text, n=2)\n",
    "                else: \n",
    "                    augmented_text = fast_augment(original_text)\n",
    "            else:\n",
    "                # Fallback:  70% synonym, 30% swap\n",
    "                if random.random() < 0.7:\n",
    "                    augmented_text = synonym_replacement(original_text, n=2)\n",
    "                else:\n",
    "                    augmented_text = fast_augment(original_text)\n",
    "            \n",
    "            # Add augmented sample\n",
    "            new_row = class_df.iloc[0].copy()\n",
    "            new_row[text_col] = augmented_text\n",
    "            augmented_rows. append(new_row)\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    augmented_df = pd. DataFrame(augmented_rows)\n",
    "    train_df = pd.concat([train_df, augmented_df], ignore_index=True)\n",
    "    train_df = train_df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nâœ… Augmentation complete!\")\n",
    "    print(f\"   Original samples: {len(train_df) - len(augmented_df):,}\")\n",
    "    print(f\"   Added samples: {len(augmented_df):,}\")\n",
    "    print(f\"   Total samples: {len(train_df):,}\")\n",
    "    print(f\"   Increase: +{len(augmented_df)/(len(train_df)-len(augmented_df))*100:.1f}%\")\n",
    "    \n",
    "    \n",
    "    # Cleanup\n",
    "    if use_backtranslation:\n",
    "        del ar_en_model, en_ar_model, ar_en_tokenizer, en_ar_tokenizer\n",
    "    \n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "# USAGE - ADD THIS AFTER PREPROCESSING\n",
    "            \n",
    "# Augment training data with back-translation\n",
    "train_df = augment_minority_classes(\n",
    "    train_df, \n",
    "    min_samples=50,      # Augment classes with < 50 samples\n",
    "    target_samples=150   # Boost to 150 samples each\n",
    ")\n",
    "\n",
    "# Show final distribution\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL CLASS DISTRIBUTION AFTER AUGMENTATION\")\n",
    "print(\"=\"*80)\n",
    "final_counts = train_df['label'].value_counts().sort_index()\n",
    "print(f\"Min samples:   {final_counts.min()} (Class {final_counts.idxmin()})\")\n",
    "print(f\"Max samples:  {final_counts.max()} (Class {final_counts.idxmax()})\")\n",
    "print(f\"Imbalance ratio: {final_counts.max() / final_counts.min():.2f}x\")\n",
    "print(f\"Total samples: {len(train_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T11:21:11.363692Z",
     "iopub.status.busy": "2025-12-18T11:21:11.362769Z",
     "iopub.status.idle": "2025-12-18T11:21:11.374853Z",
     "shell.execute_reply": "2025-12-18T11:21:11.374072Z",
     "shell.execute_reply.started": "2025-12-18T11:21:11.363659Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[4/12] COMPUTING CLASS WEIGHTS FOR IMBALANCE\n",
      "================================================================================\n",
      "\n",
      "âœ… Class weights computed!\n",
      "   Mean weight: 7.1595\n",
      "   Min weight: 0.5681\n",
      "   Max weight: 48.6951\n",
      "   Weight range: 85.71x\n"
     ]
    }
   ],
   "source": [
    "# 4. COMPUTE CLASS WEIGHTS (CLIPPED FOR STABILITY)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[4/12] COMPUTING CLASS WEIGHTS FOR IMBALANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_df['label']),\n",
    "    y=train_df['label']\n",
    ")\n",
    "\n",
    "# Clip extreme weights to prevent training instability\n",
    "class_weights = np.clip(class_weights, 0.5, 10.0)\n",
    "class_weights_tensor = torch.FloatTensor(class_weights)\n",
    "\n",
    "print(f\"\\nâœ… Class weights computed and CLIPPED!\")\n",
    "print(f\"   Original - Min: {class_weights.min():.2f}, Max: {class_weights.max():.2f}\")\n",
    "print(f\"   Clipped  - Min: {class_weights.min():.2f}, Max: {class_weights.max():.2f}\")\n",
    "print(f\"   Mean weight: {class_weights.mean():.2f}\")\n",
    "print(f\"\\nğŸ’¡ Clipped to max=10.0 to prevent gradient explosion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T11:21:14.146693Z",
     "iopub.status.busy": "2025-12-18T11:21:14.146410Z",
     "iopub.status.idle": "2025-12-18T11:21:14.167517Z",
     "shell.execute_reply": "2025-12-18T11:21:14.166965Z",
     "shell.execute_reply.started": "2025-12-18T11:21:14.146672Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[5/12] CREATING TRAIN/VALIDATION SPLIT\n",
      "================================================================================\n",
      "\n",
      "âœ… Split complete!\n",
      "   Training samples: 22360\n",
      "   Validation samples: 5591\n",
      "   Split ratio: 80/20\n"
     ]
    }
   ],
   "source": [
    "# 5. TRAIN/VALIDATION SPLIT\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[5/12] CREATING TRAIN/VALIDATION SPLIT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df['text_clean']. values,\n",
    "    train_df['label'].values,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=train_df['label']. values\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Split complete!\")\n",
    "print(f\"   Training samples: {len(train_texts)}\")\n",
    "print(f\"   Validation samples: {len(val_texts)}\")\n",
    "print(f\"   Split ratio: 80/20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T11:36:13.146357Z",
     "iopub.status.busy": "2025-12-18T11:36:13.145669Z",
     "iopub.status.idle": "2025-12-18T11:36:13.853805Z",
     "shell.execute_reply": "2025-12-18T11:36:13.853237Z",
     "shell.execute_reply.started": "2025-12-18T11:36:13.146319Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[6/12] LOADING ARABERT V2 MODEL\n",
      "================================================================================\n",
      "\n",
      "Loading tokenizer from aubmindlab/bert-large-arabertv02...\n",
      "Loading model from aubmindlab/bert-large-arabertv02...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-large-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Model loaded successfully!\n",
      "   Total parameters: 369,507,410\n",
      "   Trainable parameters: 369,507,410\n"
     ]
    }
   ],
   "source": [
    "# 6. LOAD TOKENIZER AND MODEL\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[6/12] LOADING ARABERT V2 MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nLoading tokenizer from {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"Loading model from {MODEL_NAME}...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "\n",
    "# CRITICAL: Enable gradient checkpointing (trades compute for memory)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(f\"\\nâœ… Model loaded successfully!\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p. numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T11:21:35.118536Z",
     "iopub.status.busy": "2025-12-18T11:21:35.117857Z",
     "iopub.status.idle": "2025-12-18T11:21:35.125528Z",
     "shell.execute_reply": "2025-12-18T11:21:35.124693Z",
     "shell.execute_reply.started": "2025-12-18T11:21:35.118511Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[7/12] CREATING PYTORCH DATASETS\n",
      "================================================================================\n",
      "\n",
      "âœ… Datasets created!\n",
      "   Train dataset: 22360 samples\n",
      "   Val dataset: 5591 samples\n"
     ]
    }
   ],
   "source": [
    "# 7. CREATE DATASET\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[7/12] CREATING PYTORCH DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class ArabicMedicalDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = ArabicMedicalDataset(train_texts, train_labels, tokenizer, MAX_LENGTH)\n",
    "val_dataset = ArabicMedicalDataset(val_texts, val_labels, tokenizer, MAX_LENGTH)\n",
    "\n",
    "print(f\"\\nâœ… Datasets created!\")\n",
    "print(f\"   Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"   Val dataset: {len(val_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T11:21:38.712633Z",
     "iopub.status.busy": "2025-12-18T11:21:38.712344Z",
     "iopub.status.idle": "2025-12-18T11:21:38.719361Z",
     "shell.execute_reply": "2025-12-18T11:21:38.718554Z",
     "shell.execute_reply.started": "2025-12-18T11:21:38.712611Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[8/12] SETTING UP FOCAL LOSS\n",
      "================================================================================\n",
      "âœ… Focal Loss defined with gamma=2.0 and class weights\n"
     ]
    }
   ],
   "source": [
    "# 8. DEFINE FOCAL LOSS\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[8/12] SETTING UP FOCAL LOSS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class FocalLoss(nn. Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.alpha, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss. mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "print(\"âœ… Focal Loss defined with gamma=2.0 and class weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T11:21:41.553618Z",
     "iopub.status.busy": "2025-12-18T11:21:41.553337Z",
     "iopub.status.idle": "2025-12-18T11:21:44.542721Z",
     "shell.execute_reply": "2025-12-18T11:21:44.542011Z",
     "shell.execute_reply.started": "2025-12-18T11:21:41.553596Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[9/12] CREATING CUSTOM TRAINER\n",
      "================================================================================\n",
      "âœ… Custom Trainer with Focal Loss ready!\n",
      "   âœ“ Compatible with Transformers 4.46+\n",
      "   âœ“ Supports num_items_in_batch parameter\n"
     ]
    }
   ],
   "source": [
    "# 9. CUSTOM TRAINER WITH FOCAL LOSS (UPDATED FOR NEW TRANSFORMERS)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[9/12] CREATING CUSTOM TRAINER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        if class_weights is not None:  \n",
    "            self.loss_fn = FocalLoss(\n",
    "                alpha=class_weights. to(self.args.device) if hasattr(self.args, 'device') else class_weights,\n",
    "                gamma=2.5\n",
    "            )\n",
    "        else:\n",
    "            self.loss_fn = None\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        Compute loss with Focal Loss and class weights.\n",
    "        Updated to support num_items_in_batch parameter (new in transformers 4.46+)\n",
    "        \"\"\"\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        if self.loss_fn is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute macro F1 and other metrics\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    macro_f1 = f1_score(labels, predictions, average='macro', zero_division=0)\n",
    "    micro_f1 = f1_score(labels, predictions, average='micro', zero_division=0)\n",
    "    weighted_f1 = f1_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'macro_f1': macro_f1,\n",
    "        'micro_f1': micro_f1,\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "\n",
    "print(\"âœ… Custom Trainer with Focal Loss ready!\")\n",
    "print(\"   âœ“ Compatible with Transformers 4.46+\")\n",
    "print(\"   âœ“ Supports num_items_in_batch parameter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T11:23:46.011665Z",
     "iopub.status.busy": "2025-12-18T11:23:46.011085Z",
     "iopub.status.idle": "2025-12-18T11:23:46.061374Z",
     "shell.execute_reply": "2025-12-18T11:23:46.060768Z",
     "shell.execute_reply.started": "2025-12-18T11:23:46.011637Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[10/12] CONFIGURING TRAINING\n",
      "================================================================================\n",
      "âœ… Training configuration complete!\n",
      "   Epochs: 5\n",
      "   Batch size: 12\n",
      "   Learning rate:  2e-05\n",
      "   FP16: True\n",
      "   Early stopping patience: 3 epochs\n"
     ]
    }
   ],
   "source": [
    "# 10. TRAINING CONFIGURATION\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[10/12] CONFIGURING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    logging_dir=LOGS_DIR,\n",
    "    logging_steps=50,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='macro_f1',\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_accumulation_steps=2,\n",
    "    report_to='none',\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights_tensor,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(\"âœ… Training configuration complete!\")\n",
    "print(f\"   Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Learning rate:  {LEARNING_RATE}\")\n",
    "print(f\"   FP16: {training_args.fp16}\")\n",
    "print(f\"   Early stopping patience: 3 epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T11:23:50.489144Z",
     "iopub.status.busy": "2025-12-18T11:23:50.488418Z",
     "iopub.status.idle": "2025-12-18T11:25:07.864327Z",
     "shell.execute_reply": "2025-12-18T11:25:07.863285Z",
     "shell.execute_reply.started": "2025-12-18T11:23:50.489119Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[11/12] STARTING TRAINING\n",
      "================================================================================\n",
      "\n",
      "This may take 30-60 minutes depending on your hardware.. .\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='3728' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   8/3728 00:57 < 9:55:27, 0.10 it/s, Epoch 0.01/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47/3036865923.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtraining_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtraining_end_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2207\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2546\u001b[0m                     )\n\u001b[1;32m   2547\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2548\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                     if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3795\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3797\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3799\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2572\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2573\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2574\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2575\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2576\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 11. TRAIN MODEL\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[11/12] STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nThis may take 30-60 minutes depending on your hardware.. .\\n\")\n",
    "\n",
    "training_start_time = datetime.now()\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "training_end_time = datetime.now()\n",
    "training_duration = training_end_time - training_start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T16:19:38.099838Z",
     "iopub.status.busy": "2025-12-14T16:19:38.099190Z",
     "iopub.status.idle": "2025-12-14T16:20:59.083215Z",
     "shell.execute_reply": "2025-12-14T16:20:59.082590Z",
     "shell.execute_reply.started": "2025-12-14T16:19:38.099809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(f\"\\nTraining Duration: {training_duration}\")\n",
    "\n",
    "print(f\"\\nFinal Training Metrics:\")\n",
    "for key, value in train_result.metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   {key}:  {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATING ON VALIDATION SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "eval_results = trainer. evaluate()\n",
    "\n",
    "print(f\"\\nâœ… Validation Results:\")\n",
    "for key, value in eval_results.items(): \n",
    "    if isinstance(value, float):\n",
    "        print(f\"   {key}:  {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ MACRO F1 SCORE (Competition Metric): {eval_results['eval_macro_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T16:21:18.792599Z",
     "iopub.status.busy": "2025-12-14T16:21:18.791840Z",
     "iopub.status.idle": "2025-12-14T16:21:19.693035Z",
     "shell.execute_reply": "2025-12-14T16:21:19.692276Z",
     "shell.execute_reply.started": "2025-12-14T16:21:18.792571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# KAGGLE:  SAVE MODEL TO KAGGLE DATASET\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING MODEL FOR KAGGLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# In Kaggle, save to /kaggle/working/ (this gets saved as output)\n",
    "MODEL_SAVE_DIR = '/kaggle/working/arabert_medical_model'\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\\nSaving model to {MODEL_SAVE_DIR}...\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model(MODEL_SAVE_DIR)\n",
    "tokenizer.save_pretrained(MODEL_SAVE_DIR)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'num_labels': NUM_LABELS,\n",
    "    'max_length': MAX_LENGTH,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'num_epochs':  NUM_EPOCHS,\n",
    "    'macro_f1': float(eval_results['eval_macro_f1']),\n",
    "    'micro_f1': float(eval_results['eval_micro_f1']),\n",
    "    'accuracy': float(eval_results['eval_accuracy']),\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "}\n",
    "\n",
    "with open(f'{MODEL_SAVE_DIR}/metadata.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save class weights\n",
    "np.save(f'{MODEL_SAVE_DIR}/class_weights.npy', class_weights)\n",
    "\n",
    "print(\"\\nâœ… Model saved successfully!\")\n",
    "print(f\"\\nğŸ“ Saved files in {MODEL_SAVE_DIR}:\")\n",
    "for file in os.listdir(MODEL_SAVE_DIR):\n",
    "    print(f\"   â€¢ {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T16:39:41.743788Z",
     "iopub.status.busy": "2025-12-14T16:39:41.743509Z",
     "iopub.status.idle": "2025-12-14T16:41:02.831247Z",
     "shell.execute_reply": "2025-12-14T16:41:02.830695Z",
     "shell.execute_reply.started": "2025-12-14T16:39:41.743767Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# DETAILED EVALUATION\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "predictions = trainer.predict(val_dataset)\n",
    "pred_labels = np.argmax(predictions. predictions, axis=1)\n",
    "true_labels = val_labels\n",
    "\n",
    "# Per-class F1 scores\n",
    "per_class_f1 = f1_score(true_labels, pred_labels, average=None, zero_division=0)\n",
    "\n",
    "print(f\"\\nPer-Class F1 Statistics:\")\n",
    "print(f\"   Mean: {per_class_f1.mean():.4f}\")\n",
    "print(f\"   Min: {per_class_f1.min():.4f}\")\n",
    "print(f\"   Max: {per_class_f1.max():.4f}\")\n",
    "print(f\"   Std: {per_class_f1.std():.4f}\")\n",
    "\n",
    "# Best and worst performing classes\n",
    "class_f1_dict = {i: f1 for i, f1 in enumerate(per_class_f1)}\n",
    "sorted_classes = sorted(class_f1_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nğŸ† Top 10 Best Performing Classes:\")\n",
    "for class_id, f1 in sorted_classes[:10]:\n",
    "    count = label_counts[class_id]\n",
    "    print(f\"   Class {class_id: 2d}: F1={f1:.4f}, Samples={count:4d}\")\n",
    "\n",
    "print(f\"\\nâš ï¸  Top 10 Worst Performing Classes:\")\n",
    "for class_id, f1 in sorted_classes[-10:]:\n",
    "    count = label_counts[class_id]\n",
    "    print(f\"   Class {class_id:2d}: F1={f1:.4f}, Samples={count:4d}\")\n",
    "\n",
    "# Save per-class F1 scores\n",
    "per_class_results = {\n",
    "    'class_id': list(range(NUM_LABELS)),\n",
    "    'f1_score': per_class_f1.tolist(),\n",
    "    'sample_count': [int(label_counts[i]) for i in range(NUM_LABELS)]\n",
    "}\n",
    "per_class_df = pd.DataFrame(per_class_results)\n",
    "per_class_df.to_csv(f'{MODEL_SAVE_DIR}/per_class_f1_scores.csv', index=False)\n",
    "print(f\"\\nâœ… Per-class F1 scores saved to {MODEL_SAVE_DIR}/per_class_f1_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T16:41:18.664325Z",
     "iopub.status.busy": "2025-12-14T16:41:18.663751Z",
     "iopub.status.idle": "2025-12-14T16:41:21.122145Z",
     "shell.execute_reply": "2025-12-14T16:41:21.121561Z",
     "shell.execute_reply.started": "2025-12-14T16:41:18.664301Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# VISUALIZATION\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING PERFORMANCE VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Per-class F1 scores\n",
    "ax1 = axes[0, 0]\n",
    "ax1.bar(range(NUM_LABELS), per_class_f1, color='green', alpha=0.6)\n",
    "ax1.axhline(y=per_class_f1.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {per_class_f1.mean():.4f}')\n",
    "ax1.set_title('Per-Class F1 Scores', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Class Label')\n",
    "ax1.set_ylabel('F1 Score')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "# 2. F1 Score vs Sample Count\n",
    "ax2 = axes[0, 1]\n",
    "sample_counts = [label_counts[i] for i in range(NUM_LABELS)]\n",
    "ax2.scatter(sample_counts, per_class_f1, alpha=0.6, c=per_class_f1, cmap='RdYlGn')\n",
    "ax2.set_title('F1 Score vs Sample Count', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Number of Training Samples')\n",
    "ax2.set_ylabel('F1 Score')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. F1 Score Distribution\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(per_class_f1, bins=30, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "ax3.axvline(per_class_f1.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {per_class_f1.mean():.4f}')\n",
    "ax3.axvline(np.median(per_class_f1), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(per_class_f1):.4f}')\n",
    "ax3.set_title('F1 Score Distribution', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('F1 Score')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.legend()\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Top/Bottom Classes\n",
    "ax4 = axes[1, 1]\n",
    "top_5 = sorted_classes[:5]\n",
    "bottom_5 = sorted_classes[-5:]\n",
    "combined = top_5 + bottom_5\n",
    "class_ids = [c[0] for c in combined]\n",
    "f1_scores = [c[1] for c in combined]\n",
    "colors = ['green']*5 + ['red']*5\n",
    "ax4.barh(range(len(combined)), f1_scores, color=colors, alpha=0.7)\n",
    "ax4.set_yticks(range(len(combined)))\n",
    "ax4.set_yticklabels([f'Class {c}' for c in class_ids])\n",
    "ax4.set_title('Top 5 and Bottom 5 Classes', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('F1 Score')\n",
    "ax4.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{MODEL_SAVE_DIR}/performance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… Performance visualizations saved to {MODEL_SAVE_DIR}/performance_analysis.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T16:22:43.948140Z",
     "iopub.status.busy": "2025-12-14T16:22:43.947869Z",
     "iopub.status.idle": "2025-12-14T16:22:43.954114Z",
     "shell.execute_reply": "2025-12-14T16:22:43.953253Z",
     "shell.execute_reply.started": "2025-12-14T16:22:43.948118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# EACL 2026 Abjad NLP:  Generate Submission File\n",
    "# Load trained model and create predictions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm. auto import tqdm\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EACL 2026 ABJAD NLP - SUBMISSION GENERATOR\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T16:23:12.420860Z",
     "iopub.status.busy": "2025-12-14T16:23:12.420579Z",
     "iopub.status.idle": "2025-12-14T16:23:12.426296Z",
     "shell.execute_reply": "2025-12-14T16:23:12.425418Z",
     "shell.execute_reply.started": "2025-12-14T16:23:12.420839Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "\n",
    "# Path to your saved model\n",
    "MODEL_PATH = '/kaggle/working/arabert_medical_model'\n",
    "\n",
    "# Path to test data\n",
    "TEST_DATA_PATH = '/kaggle/input/arabic/shared_task_devtest_no_label.csv'\n",
    "\n",
    "# Output submission file\n",
    "SUBMISSION_FILE = 'submission.csv'\n",
    "\n",
    "# Batch size for inference\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nğŸ–¥ï¸  Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T16:23:36.567149Z",
     "iopub.status.busy": "2025-12-14T16:23:36.566849Z",
     "iopub.status.idle": "2025-12-14T16:23:36.924233Z",
     "shell.execute_reply": "2025-12-14T16:23:36.923367Z",
     "shell.execute_reply.started": "2025-12-14T16:23:36.567125Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# KAGGLE:  LOAD MODEL FROM KAGGLE DATASET\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING MODEL FROM KAGGLE DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "MODEL_PATH = '/kaggle/working/arabert_medical_model'\n",
    "\n",
    "# Check if model exists\n",
    "import os\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f\"\\nâœ… Model directory found:  {MODEL_PATH}\")\n",
    "    print(f\"\\nğŸ“ Files in model directory:\")\n",
    "    for file in os.listdir(MODEL_PATH):\n",
    "        print(f\"   â€¢ {file}\")\n",
    "else:\n",
    "    print(f\"\\nâŒ ERROR: Model not found at {MODEL_PATH}\")\n",
    "    print(\"\\nğŸ”§ HOW TO FIX:\")\n",
    "    print(\"1. Make sure you ran the training notebook\")\n",
    "    print(\"2. Saved the output as a dataset named 'arabert-medical-model'\")\n",
    "    print(\"3. Added it as input to THIS notebook (Add Data -> Your Datasets)\")\n",
    "    raise FileNotFoundError(f\"Model not found at {MODEL_PATH}\")\n",
    "\n",
    "# Load metadata\n",
    "print(\"\\nLoading metadata...\")\n",
    "with open(f'{MODEL_PATH}/metadata.json', 'r', encoding='utf-8') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(\"\\nğŸ“‹ Model Metadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"\\nLoading tokenizer...\")\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "print(\"âœ… Tokenizer loaded!\")\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModelForSequenceClassification. from_pretrained(MODEL_PATH)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"âœ… Model loaded and moved to {device}!\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… MODEL LOADED SUCCESSFULLY - READY FOR INFERENCE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T16:23:47.851859Z",
     "iopub.status.busy": "2025-12-14T16:23:47.851237Z",
     "iopub.status.idle": "2025-12-14T16:23:48.223672Z",
     "shell.execute_reply": "2025-12-14T16:23:48.222947Z",
     "shell.execute_reply.started": "2025-12-14T16:23:47.851833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2. LOAD TEST DATA\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[2/7] LOADING TEST DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "print(f\"\\nâœ… Test data loaded!\")\n",
    "print(f\"   Shape: {test_df.shape}\")\n",
    "print(f\"   Columns: {test_df.columns. tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# Check for missing values\n",
    "if test_df.isnull().sum().sum() > 0:\n",
    "    print(f\"\\nâš ï¸  Warning: Missing values detected\")\n",
    "    print(test_df.isnull().sum())\n",
    "else:\n",
    "    print(\"\\nâœ… No missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T16:24:06.326079Z",
     "iopub.status.busy": "2025-12-14T16:24:06.325798Z",
     "iopub.status.idle": "2025-12-14T16:24:07.155864Z",
     "shell.execute_reply": "2025-12-14T16:24:07.155079Z",
     "shell.execute_reply.started": "2025-12-14T16:24:06.326056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 3. PREPROCESS TEST DATA\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[3/7] PREPROCESSING TEST DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def preprocess_arabic_text(text):\n",
    "    \"\"\"Same preprocessing as training\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove diacritics\n",
    "    text = re.sub(r'[Ù‹ÙŒÙÙÙÙÙ‘Ù’]', '', text)\n",
    "    \n",
    "    # Normalize Arabic letters\n",
    "    text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)\n",
    "    text = re.sub(r'Ù‰', 'ÙŠ', text)\n",
    "    text = re.sub(r'Ø©', 'Ù‡', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"Applying preprocessing...\")\n",
    "test_df['text_clean'] = test_df['text']. apply(preprocess_arabic_text)\n",
    "\n",
    "print(f\"âœ… Preprocessing complete!\")\n",
    "print(f\"   Total samples: {len(test_df)}\")\n",
    "\n",
    "print(\"\\nExample preprocessing:\")\n",
    "for i in range(2):\n",
    "    print(f\"\\n{i+1}. Original: {test_df['text']. iloc[i][: 120]}...\")\n",
    "    print(f\"   Cleaned:   {test_df['text_clean'].iloc[i][:120]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T16:25:44.239707Z",
     "iopub.status.busy": "2025-12-14T16:25:44.239397Z",
     "iopub.status.idle": "2025-12-14T16:25:44.247448Z",
     "shell.execute_reply": "2025-12-14T16:25:44.246635Z",
     "shell.execute_reply.started": "2025-12-14T16:25:44.239685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 4. CREATE TEST DATASET\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[4/7] CREATING TEST DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "MAX_LENGTH = metadata['max_length']\n",
    "test_dataset = TestDataset(\n",
    "    test_df['text_clean'].values,\n",
    "    tokenizer,\n",
    "    MAX_LENGTH\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"âœ… Test dataset created!\")\n",
    "print(f\"   Samples: {len(test_dataset)}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Batches: {len(test_loader)}\")\n",
    "print(f\"   Max length:  {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T16:25:49.961375Z",
     "iopub.status.busy": "2025-12-14T16:25:49.960868Z",
     "iopub.status.idle": "2025-12-14T16:34:07.979678Z",
     "shell.execute_reply": "2025-12-14T16:34:07.978902Z",
     "shell.execute_reply.started": "2025-12-14T16:25:49.961348Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 5. GENERATE PREDICTIONS\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[5/7] GENERATING PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nRunning inference.. .\\n\")\n",
    "\n",
    "all_predictions = []\n",
    "all_probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get probabilities\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_probabilities = np.array(all_probabilities)\n",
    "\n",
    "print(f\"\\nâœ… Predictions complete!\")\n",
    "print(f\"   Total predictions: {len(all_predictions)}\")\n",
    "print(f\"   Prediction range: [{all_predictions.min()}, {all_predictions.max()}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T16:34:15.256952Z",
     "iopub.status.busy": "2025-12-14T16:34:15.256670Z",
     "iopub.status.idle": "2025-12-14T16:34:16.413105Z",
     "shell.execute_reply": "2025-12-14T16:34:16.412519Z",
     "shell.execute_reply.started": "2025-12-14T16:34:15.256929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 6. ANALYZE PREDICTIONS\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[6/7] ANALYZING PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prediction statistics\n",
    "pred_counts = Counter(all_predictions)\n",
    "print(f\"\\nPrediction Statistics:\")\n",
    "print(f\"   Unique classes predicted: {len(pred_counts)}/82\")\n",
    "print(f\"   Range:  {all_predictions.min()} to {all_predictions.max()}\")\n",
    "\n",
    "# Validate predictions\n",
    "invalid_preds = (all_predictions < 0) | (all_predictions > 81)\n",
    "if invalid_preds.any():\n",
    "    print(f\"\\nâŒ ERROR: {invalid_preds.sum()} invalid predictions!\")\n",
    "else:\n",
    "    print(f\"\\nâœ… All predictions in valid range [0, 81]\")\n",
    "\n",
    "# Top predicted classes\n",
    "print(f\"\\nğŸ“Š Top 10 Most Predicted Classes:\")\n",
    "for label, count in pred_counts.most_common(10):\n",
    "    print(f\"   Class {label: 2d}: {count:5d} predictions ({count/len(all_predictions)*100:5.2f}%)\")\n",
    "\n",
    "# Confidence analysis\n",
    "max_probs = np.max(all_probabilities, axis=1)\n",
    "print(f\"\\nğŸ¯ Prediction Confidence:\")\n",
    "print(f\"   Mean:  {max_probs.mean():.4f}\")\n",
    "print(f\"   Median: {np.median(max_probs):.4f}\")\n",
    "print(f\"   Min: {max_probs.min():.4f}\")\n",
    "print(f\"   Max: {max_probs.max():.4f}\")\n",
    "\n",
    "low_conf = (max_probs < 0.5).sum()\n",
    "print(f\"   Low confidence (<0.5): {low_conf} ({low_conf/len(max_probs)*100:.2f}%)\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Prediction distribution\n",
    "ax1 = axes[0]\n",
    "pred_counts_sorted = sorted(pred_counts.items())\n",
    "labels, counts = zip(*pred_counts_sorted) if pred_counts_sorted else ([], [])\n",
    "ax1.bar(labels, counts, color='steelblue', alpha=0.7)\n",
    "ax1.set_title('Prediction Distribution', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Predicted Class')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Confidence distribution\n",
    "ax2 = axes[1]\n",
    "ax2.hist(max_probs, bins=50, color='coral', alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(max_probs.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {max_probs. mean():.3f}')\n",
    "ax2.axvline(np.median(max_probs), color='green', linestyle='--', linewidth=2, label=f'Median: {np. median(max_probs):.3f}')\n",
    "ax2.set_title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Confidence Score')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nâœ… Analysis visualization saved:  prediction_analysis.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T16:34:27.024348Z",
     "iopub.status.busy": "2025-12-14T16:34:27.023820Z",
     "iopub.status.idle": "2025-12-14T16:34:27.064128Z",
     "shell.execute_reply": "2025-12-14T16:34:27.063414Z",
     "shell.execute_reply.started": "2025-12-14T16:34:27.024306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 7. CREATE AND VALIDATE SUBMISSION\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[7/7] CREATING SUBMISSION FILE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'Id': range(len(all_predictions)),\n",
    "    'Predicted': all_predictions\n",
    "})\n",
    "\n",
    "print(f\"\\nğŸ“„ Submission Preview:\")\n",
    "print(submission_df. head(10))\n",
    "print(\"...\")\n",
    "print(submission_df.tail(10))\n",
    "\n",
    "# Validation\n",
    "print(f\"\\nğŸ” Validating submission format...\")\n",
    "\n",
    "checks = []\n",
    "\n",
    "# Check 1: Column names\n",
    "if list(submission_df.columns) == ['Id', 'Predicted']: \n",
    "    print(\"   âœ… Column names correct\")\n",
    "    checks.append(True)\n",
    "else:\n",
    "    print(f\"   âŒ Column names incorrect:  {list(submission_df.columns)}\")\n",
    "    checks.append(False)\n",
    "\n",
    "# Check 2: Id column\n",
    "if (submission_df['Id'] == range(len(submission_df))).all():\n",
    "    print(\"   âœ… Id column sequential from 0\")\n",
    "    checks.append(True)\n",
    "else:\n",
    "    print(\"   âŒ Id column not sequential\")\n",
    "    checks.append(False)\n",
    "\n",
    "# Check 3: Prediction range\n",
    "if (submission_df['Predicted'] >= 0).all() and (submission_df['Predicted'] <= 81).all():\n",
    "    print(\"   âœ… All predictions in [0, 81]\")\n",
    "    checks.append(True)\n",
    "else:\n",
    "    print(\"   âŒ Predictions out of range\")\n",
    "    checks.append(False)\n",
    "\n",
    "# Check 4: No missing values\n",
    "if submission_df.isnull().sum().sum() == 0:\n",
    "    print(\"   âœ… No missing values\")\n",
    "    checks.append(True)\n",
    "else:\n",
    "    print(f\"   âŒ Missing values found\")\n",
    "    checks.append(False)\n",
    "\n",
    "# Check 5: Data types\n",
    "if submission_df['Id'].dtype == np.int64 and submission_df['Predicted']. dtype in [np.int64, np. int32]:\n",
    "    print(\"   âœ… Data types correct\")\n",
    "    checks.append(True)\n",
    "else:\n",
    "    print(f\"   âŒ Data types incorrect\")\n",
    "    checks.append(False)\n",
    "\n",
    "# Check 6: Row count\n",
    "if len(submission_df) == len(test_df):\n",
    "    print(f\"   âœ… Row count matches ({len(submission_df)})\")\n",
    "    checks.append(True)\n",
    "else:\n",
    "    print(f\"   âŒ Row count mismatch\")\n",
    "    checks.append(False)\n",
    "\n",
    "if all(checks):\n",
    "    print(f\"\\nâœ… All validation checks passed!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Some validation checks failed!\")\n",
    "\n",
    "# Save submission file\n",
    "print(f\"\\nSaving submission to: {SUBMISSION_FILE}\")\n",
    "submission_df.to_csv(SUBMISSION_FILE, index=False)\n",
    "\n",
    "# Verify saved file\n",
    "verify_df = pd.read_csv(SUBMISSION_FILE)\n",
    "if verify_df.equals(submission_df):\n",
    "    print(f\"âœ… Submission file saved and verified!\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Warning: Verification mismatch\")\n",
    "\n",
    "file_size = len(verify_df) * 2 * 8 / 1024  # Rough estimate in KB\n",
    "print(f\"\\nğŸ“¦ File Details:\")\n",
    "print(f\"   Filename: {SUBMISSION_FILE}\")\n",
    "print(f\"   Rows: {len(submission_df)}\")\n",
    "print(f\"   Size: ~{file_size:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9020680,
     "sourceId": 14153179,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
